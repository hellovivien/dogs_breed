{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4550dc7b-7c54-4e6f-aca2-30f31dc72969",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available (YES!)\n",
      "clear GPU memory...\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ *Setup*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2 #opencv-python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import NASNetLarge\n",
    "from tensorflow.keras.applications.nasnet import preprocess_input\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.applications.resnet50 import decode_predictions, ResNet50\n",
    "import h5py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import time\n",
    "from IPython.display import display, Image\n",
    "import pickle\n",
    "from numba import cuda \n",
    "from func import * # my functions\n",
    "print(\"GPU\", \"available (YES!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")\n",
    "print(\"clear GPU memory...\")\n",
    "device = cuda.get_current_device()\n",
    "device.reset() # clear GPU memory\n",
    "step(\"Setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751ee114-1b0d-4b99-b70d-d0708c02682f",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "431baaf6-f5d1-4e54-915f-d22331ea7b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breed</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>airedale</td>\n",
       "      <td>data/train/ec3445c0c4db2d219b2377cb0eb8f3dd.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bernese_mountain_dog</td>\n",
       "      <td>data/train/bbd33c81755e1243855a1e11f44f4db6.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>otterhound</td>\n",
       "      <td>data/train/b02a0ed4c12a5cfa5be497f548de3c2b.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great_pyrenees</td>\n",
       "      <td>data/train/9ad0c52436b1b6c1a34be0b308dd0887.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kerry_blue_terrier</td>\n",
       "      <td>data/train/e24af0affe6c7a51b3e8ed9c30b090b7.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  breed                                             path\n",
       "0              airedale  data/train/ec3445c0c4db2d219b2377cb0eb8f3dd.jpg\n",
       "1  bernese_mountain_dog  data/train/bbd33c81755e1243855a1e11f44f4db6.jpg\n",
       "2            otterhound  data/train/b02a0ed4c12a5cfa5be497f548de3c2b.jpg\n",
       "3        great_pyrenees  data/train/9ad0c52436b1b6c1a34be0b308dd0887.jpg\n",
       "4    kerry_blue_terrier  data/train/e24af0affe6c7a51b3e8ed9c30b090b7.jpg"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "✅ *Data loading*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dir = \"data/train\"\n",
    "test_dir = \"data/test\"\n",
    "TRAINING_MODE = True # False = load model and history saved in file (much much faster) instead build it from zero\n",
    "df = pd.read_csv(\"data/labels.csv\")\n",
    "df=df.sample(frac=1).reset_index(drop=True) #shuffle\n",
    "df['path'] = df.id.apply(lambda x: '{}/{}.jpg'.format(train_dir, x)) # replace id by path to feed generator with flow_from_dataframe\n",
    "df.drop('id', axis=1, inplace=True)\n",
    "display(df.head())\n",
    "step(\"Data loading\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2129a008-0a14-4005-a01c-ba4ad514b93f",
   "metadata": {},
   "source": [
    "On mélange le dataframe dès le début pour éviter de le faire par la suite ce qui pourrais compliquer notre interprétation des prédictions, notemment savoir quelle image est associée à chaque prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092b5099-85aa-428c-b591-b003d99aac5d",
   "metadata": {},
   "source": [
    "## Créations des tenseurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6ab4bb-8566-4a8f-85cc-2c85d3ee0b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10018 validated image filenames belonging to 120 classes.\n",
      "Found 204 validated image filenames belonging to 120 classes.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "✅ *Tensor generator*"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'nasnet_with_aug'\n",
    "image_width = 331\n",
    "image_size = (image_width, image_width)\n",
    "\n",
    "# WITH AUGMENTATION\n",
    "generator = ImageDataGenerator(\n",
    "    validation_split=0.02,\n",
    "    horizontal_flip = True,\n",
    "    rotation_range = 20,\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    shear_range = 0.1,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode = 'nearest',\n",
    "    preprocessing_function = preprocess_input\n",
    ")\n",
    "        \n",
    "train_generator = generator.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"breed\",\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    subset=\"training\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "valid_generator = generator.flow_from_dataframe(\n",
    "    dataframe=df,\n",
    "    x_col=\"path\",\n",
    "    y_col=\"breed\",\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    subset=\"validation\",\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "step(\"Tensor generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ce04c-d4ed-4092-a969-63efdb8f02a2",
   "metadata": {},
   "source": [
    "On crée nos tenseurs à l'aide des générateurs de Tensorflow, comme nous disposons déjà d'un dataframe avec l'emplacement de nos fichiers et les labels associés on peut utiliser la fonctions **flow_from_dataframe**. On donne un nom à notre modèle qui servira à créer le fichier d'export du modèle. Toutes nos images doivent avoir **la même taille** pour avoir un traitement uniforme (un pixel = 3 données RGB). Si on a le choix on choisira 300x300 ou les recommandation du modèle pré-entrainé que l'on utilise. Il faut également **normaliser** les valeurs RGB pour cela on applique un rescale. Le batch size, c'est à dire la taille des paquet d'images qui seront soumises à chaque itération de notre entrainement est fixé à **32**, c'est la valeur qui marche la plupart du temps pour limiter le suraprentissage. **20%** de nos données seront dédiées à la validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a003401-2e1e-41c8-80f9-91847f2f7f3c",
   "metadata": {},
   "source": [
    "## Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ff0f7d-96bf-46c2-b8fa-db3991c15f7c",
   "metadata": {},
   "source": [
    "Si aucun fichier n'existe on entraine notre modèle puis on le sauvegarde ainsi que ces stats (history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74df4df-35ea-4034-be1c-68e5e453e589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Il n'existe pas de fichiers, on doit entrainer notre modèle complétement une première fois**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivien/.local/lib/python3.9/site-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:374: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "NASNet (Functional)          (None, 11, 11, 4032)      84916818  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 4032)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               483960    \n",
      "=================================================================\n",
      "Total params: 85,400,778\n",
      "Trainable params: 483,960\n",
      "Non-trainable params: 84,916,818\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "313/313 [==============================] - 200s 588ms/step - loss: 3.9063 - accuracy: 0.5503 - val_loss: 2.9474 - val_accuracy: 0.8646\n",
      "Epoch 2/25\n",
      " 39/313 [==>...........................] - ETA: 2:33 - loss: 2.8518 - accuracy: 0.8883"
     ]
    }
   ],
   "source": [
    "model_path = 'models/' + model_name + \".h5\"\n",
    "history_path = 'history/' + model_name + \".h5\"\n",
    "\n",
    "if not pathlib.Path(model_path).exists() or not pathlib.Path(history_path).exists() or TRAINING_MODE:\n",
    "    md(\"**Il n'existe pas de fichiers, on doit entrainer notre modèle complétement une première fois**\")\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        patience=2, \n",
    "        min_delta=0.001, \n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    # Setup input shape to the model\n",
    "    input_shape = [None, image_width, image_width, 3] # batch, height, width, colour channels\n",
    "\n",
    "    # Setup output shape of the model\n",
    "    output_shape = 120 # number of unique labels\n",
    "\n",
    "\n",
    "    nas_model=NASNetLarge(\n",
    "        include_top=False, \n",
    "        weights='imagenet', \n",
    "        input_shape=(image_width,image_width,3),\n",
    "    )\n",
    "\n",
    "    nas_model.trainable = False\n",
    "\n",
    "    # Setup the model layers\n",
    "    model = tf.keras.Sequential([\n",
    "        nas_model,   \n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(120, activation='softmax')\n",
    "    ])\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    opt = SGD(lr=1e-3, momentum=0.9)\n",
    "    model.compile(\n",
    "        optimizer = opt, \n",
    "        loss=\"categorical_crossentropy\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "    STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "    history = model.fit(\n",
    "        train_generator, \n",
    "        steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "        validation_data=valid_generator, \n",
    "        validation_steps=STEP_SIZE_VALID, \n",
    "        epochs=25, \n",
    "        batch_size=32, \n",
    "        callbacks=[early_stopping], \n",
    "    )\n",
    "    \n",
    "    \n",
    "    history = history.history\n",
    "    pickle.dump(history, open( history_path, \"wb\" ) )\n",
    "    save_model(model, model_path)\n",
    "    \n",
    "step(\"Making model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450a5e9-6bbb-4e84-a0ce-5e769d84b051",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7deafb36-3cdc-4f98-95fe-b5390d23742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path)\n",
    "history = pickle.load(open(history_path, 'rb'))\n",
    "plot_history(history)\n",
    "y_pred = model.predict(valid_generator, workers=16, verbose=1)\n",
    "step(\"Evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14325271-9e9c-4afa-91cf-b8a66bdc7929",
   "metadata": {},
   "source": [
    "## Prédictions sur les données de validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50d7951-a212-41a1-8fe1-67982d280d25",
   "metadata": {},
   "source": [
    "A partir des prédictions de notre modèle on va construire un dataframe avec pour chaque prédiction l'image associée et les probabilitées attribuées à chaque race de chien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4539f83-2e09-4982-83b6-2f4281b7dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_breeds = list(pd.unique(df.breed))\n",
    "unique_breeds.sort()\n",
    "breed_pred = []\n",
    "top10_pred = []\n",
    "all_preds = []\n",
    "\n",
    "for pred in y_pred:\n",
    "    breed_pred.append(unique_breeds[np.argmax(pred)])\n",
    "    top10_keys = pred.argsort()[-10:][::-1]\n",
    "    top10_values = np.sort(pred)[-10:][::-1] \n",
    "    top10_pred.append(dict(zip([unique_breeds[key] for key in top10_keys], top10_values)))\n",
    "    \n",
    "\n",
    "df_pred = pd.DataFrame({'path':valid_generator.filenames, 'breed':[unique_breeds[label_index] for label_index in valid_generator.labels], 'pred': breed_pred, 'proba': top10_pred })\n",
    "class_to_num = dict(zip(unique_breeds, range(120)))  # affenpinscher : 0\n",
    "for name in unique_breeds:  \n",
    "    df_pred[name] = y_pred[:,class_to_num[name]]\n",
    "\n",
    "md(\"**Une ligne de notre dataframe de prédictions**\")\n",
    "display(df_pred.sample().T)\n",
    "step(\"Predictions on validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba295c2-bd38-47b1-9275-c50aeaa26e15",
   "metadata": {},
   "source": [
    "## Analyse des erreurs de prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e5af47-5d69-4e0e-aabd-82d6a683f318",
   "metadata": {},
   "source": [
    "A partir de notre dataframe de prédictions on cherche à mieux visualiser les différentes érreurs de notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2650a3-94ab-46c8-bc7e-a5b25bd2ccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(row):\n",
    "    predictions = row.proba\n",
    "    if row.pred != row.breed:\n",
    "        best_score = list(predictions.values())[0]\n",
    "        return best_score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "key_cols = ['path', 'breed', 'pred', 'proba']\n",
    "\n",
    "df_pred = df_pred[key_cols].copy()\n",
    "df_pred[\"error\"] = df_pred.apply(get_error, axis=1)\n",
    "\n",
    "error_counts = len(df_pred.query(\"pred != breed\"))\n",
    "errors_percent = round(error_counts/df_pred.shape[0]*100)\n",
    "print(\"Nombre d'erreurs du modèle sur les données de validations : {}/{} ({}%)\".format(error_counts,df_pred.shape[0], errors_percent))\n",
    "print(\"Nous avons voulu savoir quelle prédictions avaient réalisé notre modèle lorsqu'il c'est trompé, pour cela on a filtrer les mauvaises prédictions et on a retenu le score de la plus forte prédiction éronnée pour produire deux classements différents. Le premier tableau montre les races de chiens les plus difficiles à prédire (moyenne de la marge d'érreur) tandis que le second montre les races qui détériorent le plus notre modèle (sommme au lieu de la moyenne)\")\n",
    "md(\"**les races les plus difficiles à prédire:**\")\n",
    "display(df_pred.groupby(\"breed\").mean(\"error\").sort_values(by=[\"error\"],ascending=False)[0:20])\n",
    "md(\"**les races dont les mauvaises prédictions impactent le plus notre modèle:**\")\n",
    "display(df_pred.groupby(\"breed\").sum(\"error\").sort_values(by=[\"error\"],ascending=False)[0:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3c5e5-7220-4c8c-b064-837fe271232f",
   "metadata": {},
   "source": [
    "### Les 50 images les plus difficiles à prédire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d472958e-926f-4c79-8a56-a8078ff23695",
   "metadata": {},
   "source": [
    "Voici les 50 images où le modèle a le moins performé. La bonne réponse est en vert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33389ec-c97a-41e8-9528-5af0295383fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred.query(\"pred != breed\").sort_values(by=[\"error\"],ascending=False).apply(plot_dog, axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81719d-9db0-4ec6-8f30-b54a55b1b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "step(\"Predictions details on validation data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b8494-cb96-4f47-8548-530a54f1b53f",
   "metadata": {},
   "source": [
    "## Création du fichier de soumission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ae9474-aafd-4850-a8c9-078011c35ecb",
   "metadata": {},
   "source": [
    "On utilise notre modèle pour prédire les races de chiens de notre échantillion de test et on enregistre ces prédictions dans un fichier que l'on pourra soumettre à Kaggle pour obtenir un score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ec5d5-6086-4a48-9220-4d53cd224f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "new_id = [el +\".jpg\" for el in test_df[\"id\"]]\n",
    "test_df[\"id\"] = new_id\n",
    "\n",
    "\n",
    "\n",
    "test_datagen=ImageDataGenerator(\n",
    "    horizontal_flip = True,\n",
    "    preprocessing_function = preprocess_input\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=test_dir,\n",
    "    x_col=\"id\",\n",
    "    y_col=None,\n",
    "    target_size=image_size,\n",
    "    batch_size=32,\n",
    "    class_mode=None,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "y_pred = model.predict(test_generator)\n",
    "\n",
    "\n",
    "df_sub = pd.read_csv('data/sample_submission.csv')\n",
    "display(df_sub.head())\n",
    "\n",
    "df_sub.iloc[:,1:] = y_pred\n",
    "display(df_sub.head())\n",
    "\n",
    "final_df = df_sub.set_index('id')\n",
    "filename = 'my_submission.csv'\n",
    "final_df.to_csv(filename)\n",
    "step(\"Fichier de soumission crée: {}\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1659348-eef3-4409-bb1e-771299540692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"clear GPU memory...\")\n",
    "device.reset() # clear GPU memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
